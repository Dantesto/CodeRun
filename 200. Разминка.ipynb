{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ada82184-8080-4be0-b379-e8010e6e6610",
   "metadata": {},
   "source": [
    "# Условие"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbfe7f3-9a46-4555-b301-0f144c837a82",
   "metadata": {},
   "source": [
    "Дан числовой сет. Таргет-функция $f:R^{100}\\rightarrow R$ линейна, зависит от всех признаков. Требуется найти эту функцию."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd9e125-1320-463b-a77a-b3ec55487775",
   "metadata": {},
   "source": [
    "# Решение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57295c9-980f-4cb0-a770-9a0f1ad4d705",
   "metadata": {},
   "source": [
    "Нам явно обозначили, что функция зависит линейно от всех признаков, значит решение можно выполнить без градиентных методов, а просто аналитически. Пусть $X$ - матрица $[n, 100]$, набор из $n$ данных, а $y$ - вектор-столбец $[n, 1]$, таргеты.\n",
    "$$f(x)=xw$$\n",
    "$$Xw=y$$\n",
    "найти функцию линейную $f$ таким образом значит найти $w$. Решаем методом наименьших квадратов.\n",
    "$$L(w)=||Xw-y||^2=<Xw-y, Xw-y>$$\n",
    "Хотим минимизировать сумму квадратов разности функции $Xw$ и таргета $y$. Для этого просто ищем в какой точке $w$ производная $L$ равна нулю. Ищем через дифференциал.\n",
    "$$[D_{w_0}L](h)=<[D_{w_0}(Xw-y)](h), Xw-y>+<Xw-y, [D_{w_0}(Xw-y)](h)>=(*)$$\n",
    "$$[D_{w_0}(Xw-y)](h)=X(w_0+h)-y-(Xw_0-y)=Xh$$\n",
    "$$(*)=<Xh, Xw-y>+<Xw-y, Xh>=<2(Xw-y), Xh>=(**)$$\n",
    "Чтобы найти дифференциал, стандартная форма которого в скалярном произведении $[D_{x_0}f](h)=<\\nabla_{x_0} f, h>$, всомним, что $<u, v>=u^Tv$.\n",
    "$$(**)=2(Xw-y)^TXh=2(((Xw-y)^TX)^T)^Th=<2((Xw-y)^TX)^T, h>=(***)$$\n",
    "По свойству транспонирования $(AB)^T=B^TA^T$.\n",
    "$$(***)=<2X^T(Xw-y), h>$$\n",
    "Значит производная $\\nabla_{w}L=2X^T(Xw-y)$.\n",
    "\n",
    "Вообще, если знать заранее свойство $<A, BC>=<B^TA, C>$, то уже после $(**)$ можно сразу получить ответ.\n",
    "\n",
    "Теперь приравняем производную к нулю и выразим $w$.\n",
    "$$2X^T(Xw-y)=0$$\n",
    "$$X^T(Xw-y)=X^TXw-X^Ty$$\n",
    "$$X^TXw=X^Ty$$\n",
    "$$w=(X^TX)^{-1}X^Ty$$\n",
    "Как нашли $w$, просто считаем предсказания на тесте.\n",
    "\n",
    "**Важно** заметить, что в условии сказано, что функция $f$ является линейным отображением из $R^{100}$, и это значит, что она не зависит от свободного члена, поэтому мы не рассматриваем $f=xw+b$.\n",
    "\n",
    "**Важно 2.** Вообще в numpy есть встроенный метод наименьших квадратов lstsq (least square). Он работает точнее."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999240e2-6a5e-4d9e-91ee-2b6e0f928708",
   "metadata": {},
   "source": [
    "# Код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe8d615c-b1d8-423d-a95d-a76e66d46a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv('200train.tsv', delimiter='\\t', header=None)\n",
    "X = np.array(df_train.iloc[:, :-1])\n",
    "#X = np.hstack((np.ones((X.shape[0], 1)), X)) # Не добавляем столбец из 1, означающий своодный член b\n",
    "y = np.array(df_train.iloc[:, -1])\n",
    "\n",
    "w = np.linalg.pinv(X.T @ X) @ X.T @ y\n",
    "\n",
    "df_test = pd.read_csv('200test.tsv', delimiter='\\t', header=None)\n",
    "X_test = np.array(df_test)\n",
    "#X_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n",
    "y_pred = X_test @ w\n",
    "with open('200answer.tsv', 'w') as f:\n",
    "    f.write('\\n'.join(map(str, y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

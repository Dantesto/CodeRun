{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4e3fddd-0adc-4259-8e53-2e0bff947936",
   "metadata": {},
   "source": [
    "# Условие"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa2db65-8453-485c-a4bd-232293a03ae5",
   "metadata": {},
   "source": [
    "Дана выборка линейно разделимая выборка $X=\\{x_1,,x_2,\\dots,x_n\\}:\\ x_i\\in \\mathbb{R}^m$, $Y=\\{y_1,y_2,\\dots,y_n\\}: y_i=\\pm 1$.\n",
    "<br>\n",
    "Необходимо найти разделяющую гиперплоскость, проходящую через начало координат, т. е. найти такой вектор $w$:\n",
    "$$sign\\left(x_iw\\right)=sign\\left(\\sum_{j=1}^mx_{ij}w_j\\right)=y_i,\\forall i$$\n",
    "где $x_i$ - вектор-строка, $w$ - вектор-столбец, $y_i$ - число."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0876ac-2aac-40fc-88c3-48b4d148b8cb",
   "metadata": {},
   "source": [
    "# Решение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dba304d-cdc1-423b-a05d-c52cff269550",
   "metadata": {},
   "source": [
    "**Логистическая регрессия**\n",
    "<br>\n",
    "Просто напишем логистическую регрессию, выведем в ответ компоненты гиперплоскости. Используем батчи для ускорения. Вместо количества эпох условием выхода из цикла будет проверка того, что каждый элемент выборки оказался со своей стороны от разделяющей гиперплоскости. Это нестабильно в случае, если выборка не линейно разделима, но это не наш случай.\n",
    "\n",
    "**Персептрон**\n",
    "<br>\n",
    "Для нахождения разделяющей поверхности проще использовать алгоритм персептрона (Novikoff, 1962). Это неградиентный алгоритм, который не работает, если выборка линейно не разделима. Но если выборка линейно разделима, проще использовать этот алгоритм, так как его и написать гораздо проще, и настраивать не нужно гиперпараметры.\n",
    "<br>\n",
    "Этот алгоритм идеально подходит под олимпиадные или учебные задачи, но в реальности, где выборка почти никогда не может быть идеально разделена даже нелинейно, используют градиентные способы аппроксимации фукнции ошибки. Это позволяет в общем улучшать модель, плавно уменьшая ошибку, в то время как ошибка перцептрона скачет туда-сюда, и планомерно уменьшить её не получится - либо ошибка 0, либо непонятно какая.\n",
    "<br>\n",
    "Общая мысль алгоритма такова. Пусть имеется вектор $w$, при этом не для всех выполняется условие $y_i * (x_iw)>0$ ($y_i$ - число, остальное векторы).\n",
    "<br>\n",
    "Тогда, возьмём один из таких $x_i$, для которых условие не выполняется и прибавим $y_i * x_i$ к $w:=w+y_i * x_i$, то есть перетянем $w$ в сторону $x_i$ (если $y_i=1$, иначе - в противоположную сторону). После этого угол между $w$ и $x_i$ уменьшится (уменьшится между $w$ и $-x_i$ при $y_i=-1$), а значит приблизится ситуация, когда угол между ними станет $<90$. А если угол меньше 90, то и скалярное произведение (это наш случай - $x_iw$) будет больше нуля, так как скалярное произведение можно выразить через косинус, который больше 0 при угле меньше 90.\n",
    "<br>\n",
    "Novikoff доказал теорему о том, что если так постоянно перетягивать вектор $w$ к тем $x_i$, на которых он ошибается, через конечное число итераций алгоритм сойдётся."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0a9c3e-5785-4630-b98e-67862f82eeca",
   "metadata": {},
   "source": [
    "# Код"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e078f2f5-63e5-4955-8197-543bc15452a8",
   "metadata": {},
   "source": [
    "**Логистическая регрессия**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee5a13b6-bfef-48e0-8b53-2b101531b6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 2 1\n",
      " -1 -1\n",
      " 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30896112736195985\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "gen_rand_rng = np.random.default_rng(seed=42)\n",
    "\n",
    "def gen_batches(X, Y, batch_size):\n",
    "    perm = gen_rand_rng.permutation(X.shape[0])\n",
    "    X_perm, Y_perm = X[perm], Y[perm]\n",
    "    for i in range(0, X.shape[0], batch_size):\n",
    "        yield X_perm[i:min(i + batch_size, X.shape[0])], Y_perm[i:min(i + batch_size, X.shape[0])]\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.W = gen_rand_rng.standard_normal((X.shape[1], Y.shape[1]))\n",
    "        self.batch_size = max(round(self.X.shape[0]**0.5), 100) # batches_size = sqrt(N)\n",
    "\n",
    "    def _isDivided(self):\n",
    "        return np.all(np.sign(self.X @ self.W) == np.sign(self.Y-0.5))\n",
    "\n",
    "    def _predict(self, X):\n",
    "        return 1 / (1 + np.exp(-X @ self.W))\n",
    "\n",
    "    def fit(self):\n",
    "        while not self._isDivided():\n",
    "            for X_batch, Y_batch in gen_batches(self.X, self.Y, self.batch_size):\n",
    "                Y_pred = self._predict(X_batch)\n",
    "                grad = (X_batch.T @ (Y_pred - Y_batch)) / X_batch.shape[0]\n",
    "                self.W -= 0.01 * grad\n",
    "                # Важно не проверять _isDivided() внутри цикла for, потому что такая проверка работает за O(N), тогда если батчей sqrt(N),\n",
    "                # одна эпоха вместо O(N) действий выполняет O(N^1.5), что дольше, чем просто считать градиент по всей выборки без батчей\n",
    "            \n",
    "n, m = [int(x) for x in input().split()]\n",
    "X, Y = [], []\n",
    "for i in range(n):\n",
    "    data = [float(x) for x in input().split()]\n",
    "    X.append(data[:-1])\n",
    "    Y.append(1 if data[-1] == 1 else 0)\n",
    "X = np.array(X)\n",
    "Y = np.array(Y).reshape((-1, 1))\n",
    "model = LogisticRegression(X, Y)\n",
    "model.fit()\n",
    "print(*model.W.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b439d68-a02d-4930-b6a8-9c743b68874b",
   "metadata": {},
   "source": [
    "**Персептрон**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f7357f6-a8d9-483a-a262-d0fc7e62c3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 2 2\n",
      " 0 1 1\n",
      " 1 1 -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.0 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "n, m = [int(x) for x in input().split()]\n",
    "X, Y = [], []\n",
    "for i in range(n):\n",
    "    data = [float(x) for x in input().split()]\n",
    "    X.append(data[:-1])\n",
    "    Y.append(data[-1])\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "W = np.zeros(m)\n",
    "error_appeared = True\n",
    "while error_appeared:\n",
    "    error_appeared = False\n",
    "    for i in range(n):\n",
    "        if X[i] @ W * Y[i] <= 0:\n",
    "            error_appeared = True\n",
    "            W += X[i].T * Y[i]\n",
    "print(*W)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
